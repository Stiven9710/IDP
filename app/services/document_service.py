"""
Servicio principal de procesamiento de documentos IDP
"""

import asyncio
import logging
import time
import uuid
from datetime import datetime
from typing import Dict, Any, List, Optional
from urllib.parse import urlparse
import httpx

from app.core.config import settings
from app.core.security_config import security_config
from app.models.request import DocumentProcessingRequest
from app.models.response import (
    DocumentProcessingResponse, 
    ProcessingStatus, 
    ProcessingMode,
    ExtractionField,
    ProcessingSummary
)
from app.services.ai_orchestrator import AIOrchestrator
from app.services.storage_service import StorageService
from app.services.queue_storage_service import QueueStorageService
from app.services.cosmos_service import CosmosService
from app.services.blob_storage_service import BlobStorageService
from app.utils.helpers import get_file_size_from_url, is_supported_format

# Configurar logging
logger = logging.getLogger(__name__)


class DocumentService:
    """Servicio principal para el procesamiento de documentos"""
    
    def __init__(self):
        """Inicializar el servicio de documentos"""
        self.ai_orchestrator = AIOrchestrator()
        self.storage_service = StorageService()
        self.queue_service = QueueStorageService()
        self.cosmos_service = CosmosService()
        self.blob_storage_service = BlobStorageService()
        
        # Umbral para procesamiento asÃ­ncrono (10MB segÃºn requerimiento)
        self.async_threshold_mb = 10.0        
        # ConfiguraciÃ³n de seguridad para URLs externas (usando configuraciÃ³n centralizada)
        self.security_config = security_config
        
        # Logging de configuraciÃ³n de seguridad
        security_summary = self.security_config.get_security_summary()
        logger.info(f"ğŸ”’ ConfiguraciÃ³n de seguridad cargada:")
        logger.info(f"   ğŸŒ Dominios permitidos: {security_summary['allowed_domains_count']}")
        logger.info(f"   ğŸš« Dominios bloqueados: {security_summary['blocked_domains_count']}")
        logger.info(f"   âš ï¸ Extensiones peligrosas: {security_summary['dangerous_extensions_count']}")
        logger.info(f"   â±ï¸ Timeout de descarga: {security_summary['download_timeout']}")
        logger.info(f"   ğŸ“ TamaÃ±o mÃ¡ximo: {security_summary['max_file_size']}")
        logger.info(f"   ğŸ”„ MÃ¡ximo de redirecciones: {security_summary['max_redirects']}")
        logger.info(f"   ğŸš¦ Rate limiting: {security_summary['rate_limiting']['per_minute']}/min, {security_summary['rate_limiting']['per_hour']}/h")
        
        logger.info("ğŸš€ DocumentService inicializado correctamente")
    
    async def validate_external_url(self, url: str) -> Dict[str, Any]:
        """
        Valida que la URL externa sea segura para procesar
        
        Args:
            url: URL del documento a validar
            
        Returns:
            Dict con resultado de validaciÃ³n y detalles
        """
        logger.info(f"ğŸ”’ Validando seguridad de URL: {url}")
        
        try:
            # Verificar que sea una URL vÃ¡lida
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                logger.warning(f"âš ï¸ URL invÃ¡lida: {url}")
                return {
                    'is_valid': False,
                    'reason': 'URL invÃ¡lida',
                    'details': 'La URL no tiene un formato vÃ¡lido'
                }
            
            # Verificar que sea HTTPS
            if parsed_url.scheme.lower() != 'https':
                logger.warning(f"âš ï¸ Protocolo no seguro: {parsed_url.scheme}")
                return {
                    'is_valid': False,
                    'reason': 'Protocolo no seguro',
                    'details': f'Solo se permiten URLs HTTPS, recibido: {parsed_url.scheme}'
                }
            
            # Extraer dominio
            domain = parsed_url.netloc.lower()
            logger.info(f"ğŸŒ Dominio extraÃ­do: {domain}")
            
            # Verificar dominios bloqueados
            if self.security_config.is_domain_allowed(domain) == False:
                logger.warning(f"ğŸš« Dominio bloqueado o no permitido: {domain}")
                return {
                    'is_valid': False,
                    'reason': 'Dominio no permitido',
                    'details': f'El dominio {domain} no estÃ¡ en la lista blanca o estÃ¡ bloqueado'
                }
            
            logger.info(f"âœ… Dominio permitido: {domain}")
            
            # Verificar que no sea una URL local o privada
            if any(private_ip in domain for private_ip in ['159.203.149.247', '127.0.0.1', '192.168.', '10.', '172.']):
                logger.warning(f"ğŸš« URL local/privada detectada: {domain}")
                return {
                    'is_valid': False,
                    'reason': 'URL local/privada',
                    'details': 'No se permiten URLs locales o de redes privadas'
                }
            
            logger.info(f"âœ… URL validada exitosamente: {url}")
            return {
                'is_valid': True,
                'reason': 'URL segura',
                'details': f'Dominio {domain} validado correctamente'
            }
            
        except Exception as e:
            logger.error(f"âŒ Error validando URL: {str(e)}")
            return {
                'is_valid': False,
                'reason': 'Error de validaciÃ³n',
                'details': f'Error interno: {str(e)}'
            }
    
    async def validate_document_security(self, url: str, file_size_mb: float) -> Dict[str, Any]:
        """
        Valida la seguridad del documento antes del procesamiento
        
        Args:
            url: URL del documento
            file_size_mb: TamaÃ±o del archivo en MB
            
        Returns:
            Dict con resultado de validaciÃ³n de seguridad
        """
        logger.info(f"ğŸ”’ Validando seguridad del documento: {url}")
        
        # ğŸ” DETECCIÃ“N INTELIGENTE: Si no es una URL vÃ¡lida, saltar validaciÃ³n
        if not url.startswith(('http://', 'https://')):
            logger.info(f"ğŸ“ Archivo local detectado en validate_document_security: {url}")
            logger.info(f"   âœ… Saltando validaciones de URL externa")
            return {
                'is_valid': True,
                'reason': 'Archivo local',
                'details': 'Archivo local detectado, validaciones de URL omitidas'
            }
        
        # Validar URL solo si es una URL externa
        url_validation = await self.validate_external_url(url)
        if not url_validation['is_valid']:
            return url_validation
        
        # Validar tamaÃ±o del archivo
        if file_size_mb > self.security_config.MAX_FILE_SIZE_MB:
            logger.warning(f"âš ï¸ Archivo demasiado grande: {file_size_mb:.2f} MB > {self.security_config.MAX_FILE_SIZE_MB} MB")
            return {
                'is_valid': False,
                'reason': 'Archivo demasiado grande',
                'details': f'El archivo excede el tamaÃ±o mÃ¡ximo permitido: {file_size_mb:.2f} MB > {self.security_config.MAX_FILE_SIZE_MB} MB'
            }
        
        # Validar que no sea un archivo ejecutable
        file_extension = url.lower().split('.')[-1] if '.' in url else ''
        
        if self.security_config.is_extension_dangerous(file_extension):
            logger.warning(f"ğŸš« ExtensiÃ³n peligrosa detectada: {file_extension}")
            return {
                'is_valid': False,
                'reason': 'ExtensiÃ³n peligrosa',
                'details': f'No se permiten archivos con extensiÃ³n {file_extension}'
            }
        
        logger.info(f"âœ… Documento validado como seguro")
        return {
            'is_valid': True,
            'reason': 'Documento seguro',
            'details': 'Todas las validaciones de seguridad pasaron'
        }
    
    async def _cleanup_document_if_needed(self, job_id: str, blob_path: str, persistencia: bool) -> None:
        """
        Limpia el documento del storage segÃºn la configuraciÃ³n de persistencia
        
        Args:
            job_id: ID del trabajo de procesamiento
            blob_path: Ruta del blob en Azure Storage
            persistencia: Si es True, conserva el documento; si es False, lo elimina
        """
        try:
            if not persistencia:
                logger.info(f"ğŸ§¹ LIMPIEZA AUTOMÃTICA ACTIVADA para job {job_id}")
                logger.info(f"   ğŸ“ Blob a eliminar: {blob_path}")
                logger.info(f"   ğŸ¯ RazÃ³n: persistencia=False")
                
                # Eliminar documento del storage
                deletion_result = await self.blob_storage_service.delete_document(blob_path)
                
                if deletion_result:
                    logger.info(f"âœ… Documento eliminado exitosamente del storage")
                    logger.info(f"   ğŸ—‘ï¸ Blob eliminado: {blob_path}")
                    logger.info(f"   ğŸ†” Job: {job_id}")
                else:
                    logger.warning(f"âš ï¸ No se pudo eliminar el documento del storage")
                    logger.warning(f"   ğŸ“ Blob: {blob_path}")
                    logger.warning(f"   ğŸ†” Job: {job_id}")
            else:
                logger.info(f"ğŸ’¾ PERSISTENCIA ACTIVADA para job {job_id}")
                logger.info(f"   ğŸ“ Documento conservado en: {blob_path}")
                logger.info(f"   ğŸ¯ RazÃ³n: persistencia=True")
                
        except Exception as e:
            logger.error(f"âŒ Error en limpieza automÃ¡tica del documento: {str(e)}")
            logger.error(f"   ğŸ†” Job: {job_id}")
            logger.error(f"   ğŸ“ Blob: {blob_path}")
            logger.error(f"   ğŸ¯ Persistencia: {persistencia}")
    
    async def process_document(
        self, 
        request: DocumentProcessingRequest
    ) -> DocumentProcessingResponse:
        """
        Procesar un documento de forma sÃ­ncrona o asÃ­ncrona segÃºn su tamaÃ±o
        
        Args:
            request: Request de procesamiento del documento
            
        Returns:
            Response con el resultado del procesamiento
        """
        start_time = time.time()
        job_id = str(uuid.uuid4())
        
        logger.info(f"ğŸ“„ Iniciando procesamiento del documento: {job_id}")
        logger.info(f"ğŸ”— URL del documento: {request.document_path}")
        logger.info(f"ğŸ¯ Modo de procesamiento: {request.processing_mode}")
        logger.info(f"ğŸ“ NÃºmero de campos a extraer: {len(request.fields)}")
        
        try:
            # ğŸ”’ VALIDACIÃ“N DE SEGURIDAD PARA URLs EXTERNAS
            # ğŸ” DETECCIÃ“N INTELIGENTE: Archivo local vs URL externa
            if hasattr(request, 'file') and request.file:
                # ğŸ“ ARCHIVO LOCAL - Saltar validaciones de URL
                logger.info(f"ğŸ“ Archivo local detectado: {request.file.filename}")
                logger.info(f"   âœ… Saltando validaciones de URL externa")
                
                # Leer archivo para obtener tamaÃ±o
                file_content = request.file.file.read()
                file_size_mb = len(file_content) / (1024 * 1024)
                request.file.file.seek(0)  # Resetear posiciÃ³n del archivo
                
                logger.info(f"ğŸ“ TamaÃ±o del archivo: {file_size_mb:.2f} MB")
                logger.info("ğŸ”’ VALIDACIONES DE SEGURIDAD COMPLETADAS (archivo local)")
                logger.info("="*80)
                
            else:
                # ğŸŒ URL EXTERNA - Aplicar validaciones de seguridad
                logger.info("ğŸ”’ INICIANDO VALIDACIONES DE SEGURIDAD (URL externa)")
                
                # Validar formato del documento
                if not self._validate_document_format(str(request.document_path)):
                    raise ValueError(f"Formato de documento no soportado: {request.document_path}")
                
                # Obtener tamaÃ±o del archivo
                file_size_mb = await self._get_document_size(str(request.document_path))
                logger.info(f"ğŸ“ TamaÃ±o del documento: {file_size_mb:.2f} MB")
                
                # Validar seguridad del documento (URL + tamaÃ±o + extensiÃ³n)
                security_validation = await self.validate_document_security(
                    str(request.document_path), 
                    file_size_mb
                )
                
                if not security_validation['is_valid']:
                    logger.error(f"ğŸš« VALIDACIÃ“N DE SEGURIDAD FALLIDA: {security_validation['reason']}")
                    logger.error(f"ğŸ“‹ Detalles: {security_validation['details']}")
                    raise ValueError(f"Documento rechazado por seguridad: {security_validation['reason']} - {security_validation['details']}")
                
                logger.info(f"âœ… VALIDACIÃ“N DE SEGURIDAD EXITOSA: {security_validation['reason']}")
                logger.info(f"ğŸ“‹ Detalles: {security_validation['details']}")
                logger.info("ğŸ”’ VALIDACIONES DE SEGURIDAD COMPLETADAS")
                logger.info("="*80)
            
            # Decidir si procesar de forma sÃ­ncrona o asÃ­ncrona
            logger.info(f"ğŸ” DECISIÃ“N DE PROCESAMIENTO:")
            logger.info(f"   ğŸ“ TamaÃ±o del documento: {file_size_mb:.2f} MB")
            logger.info(f"   ğŸ¯ Umbral asÃ­ncrono: {self.async_threshold_mb} MB")
            logger.info(f"   ğŸ“Š ComparaciÃ³n: {file_size_mb:.2f} MB {'<=' if file_size_mb <= self.async_threshold_mb else '>'} {self.async_threshold_mb} MB")
            
            if file_size_mb <= self.async_threshold_mb:
                logger.info("âš¡ PROCESAMIENTO SÃNCRONO SELECCIONADO (documento pequeÃ±o)")
                logger.info(f"   ğŸ“ TamaÃ±o: {file_size_mb:.2f} MB <= {self.async_threshold_mb} MB")
                logger.info(f"   ğŸ¯ MÃ©todo: _process_synchronously")
                logger.info(f"   " + "="*80)
                return await self._process_synchronously(
                    job_id, request, file_size_mb, start_time
                )
            else:
                logger.info("ğŸ”„ PROCESAMIENTO ASÃNCRONO SELECCIONADO (documento grande)")
                logger.info(f"   ğŸ“ TamaÃ±o: {file_size_mb:.2f} MB > {self.async_threshold_mb} MB")
                logger.info(f"   ğŸ¯ MÃ©todo: _process_asynchronously")
                logger.info(f"   " + "="*80)
                return await self._process_asynchronously(
                    job_id, request, file_size_mb, start_time
                )
                
        except Exception as e:
            logger.error(f"âŒ Error en el procesamiento del documento {job_id}: {str(e)}")
            raise
    
    async def _process_synchronously(
        self,
        job_id: str,
        request: DocumentProcessingRequest,
        file_size_mb: float,
        start_time: float
    ) -> DocumentProcessingResponse:
        """Procesar documento de forma sÃ­ncrona"""
        logger.info(f"âš¡ Iniciando procesamiento sÃ­ncrono para job {job_id}")
        logger.info(f"ğŸ“Š TamaÃ±o del documento: {file_size_mb:.2f} MB")
        logger.info(f"ğŸ¯ Modo de procesamiento: {request.processing_mode}")
        logger.info(f"ğŸ“‹ Campos a extraer: {len(request.fields)}")
        
        try:
            # Descargar documento temporalmente
            document_content = await self._download_document(str(request.document_path))
            logger.info(f"ğŸ“¥ Documento descargado: {len(document_content)} bytes")
            
            # Procesar con IA
            logger.info(f"ğŸ¤– Iniciando procesamiento con IA...")
            extraction_result = await self.ai_orchestrator.process_document(
                document_content=document_content,
                fields=request.fields,
                prompt_general=request.prompt_general,
                processing_mode=request.processing_mode
            )
            logger.info(f"âœ… Procesamiento con IA completado")
            logger.info(f"ğŸ“Š Campos extraÃ­dos: {len(extraction_result.extraction_data)}")
            
            # Calcular tiempo de procesamiento
            processing_time_ms = int((time.time() - start_time) * 1000)
            logger.info(f"â±ï¸ Tiempo total de procesamiento: {processing_time_ms}ms")
            
            # ===== INICIO: GESTIÃ“N DE STORAGE Y PERSISTENCIA =====
            logger.info(f"ğŸ’¾ GESTIÃ“N DE STORAGE Y PERSISTENCIA para job {job_id}")
            logger.info(f"   ğŸ¯ ConfiguraciÃ³n de persistencia: {request.persistencia}")
            
            # Subir documento al storage para procesamiento
            filename = str(request.document_path).split("/")[-1] if "/" in str(request.document_path) else str(request.document_path)
            
            # Limpiar metadatos para Azure
            clean_metadata = {}
            if request.metadata:
                for key, value in request.metadata.items():
                    if value is not None:
                        clean_metadata[key] = str(value)
            
            # Agregar metadatos de persistencia
            clean_metadata['persistencia'] = str(request.persistencia)
            clean_metadata['job_id'] = job_id
            clean_metadata['processing_mode'] = request.processing_mode
            
            blob_path = await self.blob_storage_service.upload_document(
                file_content=document_content,
                filename=filename,
                job_id=job_id,
                metadata=clean_metadata
            )
            
            if not blob_path:
                logger.warning(f"âš ï¸ No se pudo subir el documento al storage, pero continuando con el procesamiento")
                blob_path = f"error_upload_{job_id}"
            else:
                logger.info(f"ğŸ“¦ Documento subido al storage: {blob_path}")
                logger.info(f"   ğŸ¯ Persistencia configurada: {request.persistencia}")
            
            # ===== FIN: GESTIÃ“N DE STORAGE Y PERSISTENCIA =====
            
            # Crear response
            response = DocumentProcessingResponse(
                job_id=job_id,
                extraction_data=extraction_result.extraction_data,
                processing_summary=ProcessingSummary(
                    processing_status=ProcessingStatus.COMPLETED,
                    processing_time_ms=processing_time_ms,
                    file_size_mb=file_size_mb,
                    strategy_used=ProcessingMode(request.processing_mode),
                    timestamp=datetime.utcnow(),
                    pages_processed=extraction_result.pages_processed,
                    review_flags=extraction_result.review_flags
                ),
                message="Documento procesado exitosamente de forma sÃ­ncrona",
                correlation_id=request.metadata.get("correlation_id") if request.metadata else None
            )
            
            # ===== INICIO: GUARDADO EN COSMOS DB =====
            logger.info(f"ğŸ—„ï¸ Iniciando guardado en Azure Cosmos DB...")
            
            # Guardar informaciÃ³n del documento en Cosmos DB
            document_info = {
                "filename": request.document_path.split("/")[-1] if "/" in request.document_path else request.document_path,
                "file_size_mb": file_size_mb,
                "file_type": request.document_path.split(".")[-1] if "." in request.document_path else "unknown",
                "status": "processed",
                "processing_mode": request.processing_mode,
                "correlation_id": request.metadata.get("correlation_id") if request.metadata else None,
                "job_id": job_id,
                "created_at": datetime.utcnow().isoformat(),
                "ai_processing_time_ms": processing_time_ms
            }
            
            logger.info(f"ğŸ“„ Guardando informaciÃ³n del documento en Cosmos DB...")
            logger.info(f"   ğŸ“ Nombre: {document_info['filename']}")
            logger.info(f"   ğŸ“ TamaÃ±o: {document_info['file_size_mb']:.2f} MB")
            logger.info(f"   ğŸ¯ Modo: {document_info['processing_mode']}")
            
            doc_id = await self.cosmos_service.save_document(document_info)
            if doc_id:
                logger.info(f"âœ… Documento guardado exitosamente en Cosmos DB")
                logger.info(f"   ğŸ†” Document ID: {doc_id}")
                logger.info(f"   ğŸ“ Container: documents")
                
                # Guardar resultado de extracciÃ³n
                extraction_data = {
                    "document_id": doc_id,
                    "extraction_date": datetime.utcnow().isoformat(),
                    "processing_time_ms": processing_time_ms,
                    "strategy_used": request.processing_mode,
                    "extraction_data": [field.dict() for field in extraction_result.extraction_data],
                    "processing_summary": {
                        "processing_status": "completed",
                        "pages_processed": extraction_result.pages_processed,
                        "review_flags": extraction_result.review_flags
                    },
                    "job_id": job_id,
                    "filename": document_info['filename'],
                    "fields_extracted": len(extraction_result.extraction_data)
                }
                
                logger.info(f"ğŸ” Guardando resultado de extracciÃ³n en Cosmos DB...")
                logger.info(f"   ğŸ“Š Campos extraÃ­dos: {extraction_data['fields_extracted']}")
                logger.info(f"   ğŸ¯ Estrategia: {extraction_data['strategy_used']}")
                
                ext_id = await self.cosmos_service.save_extraction_result(extraction_data)
                if ext_id:
                    logger.info(f"âœ… ExtracciÃ³n guardada exitosamente en Cosmos DB")
                    logger.info(f"   ğŸ†” Extraction ID: {ext_id}")
                    logger.info(f"   ğŸ“ Container: extractions")
                    logger.info(f"   ğŸ”— Vinculada al documento: {doc_id}")
                else:
                    logger.error(f"âŒ ERROR: No se pudo guardar la extracciÃ³n en Cosmos DB")
                    logger.error(f"   ğŸ“„ Document ID: {doc_id}")
                    logger.error(f"   ğŸ” Datos de extracciÃ³n: {len(extraction_data)} campos")
            else:
                logger.error(f"âŒ ERROR: No se pudo guardar el documento en Cosmos DB")
                logger.error(f"   ğŸ“ Nombre: {document_info['filename']}")
                logger.error(f"   ğŸ“ TamaÃ±o: {document_info['file_size_mb']:.2f} MB")
            
            # ===== FIN: GUARDADO EN COSMOS DB =====
            
            # Crear trabajo de procesamiento
            job_info = {
                "job_id": job_id,
                "document_name": document_info['filename'],
                "processing_mode": request.processing_mode,
                "status": "completed",
                "created_at": datetime.utcnow().isoformat(),
                "completed_at": datetime.utcnow().isoformat(),
                "processing_time_ms": processing_time_ms,
                "fields_extracted": len(extraction_result.extraction_data),
                "document_id": doc_id if doc_id else None,
                "extraction_id": ext_id if 'ext_id' in locals() else None
            }
            
            logger.info(f"âš™ï¸ Guardando informaciÃ³n del trabajo en Cosmos DB...")
            job_db_id = await self.cosmos_service.create_processing_job(job_info)
            if job_db_id:
                logger.info(f"âœ… Trabajo guardado exitosamente en Cosmos DB")
                logger.info(f"   ğŸ†” Job DB ID: {job_db_id}")
                logger.info(f"   ğŸ“ Container: processing_jobs")
            else:
                logger.warning(f"âš ï¸ No se pudo guardar el trabajo en Cosmos DB")
            
            # Resumen final
            logger.info(f"ğŸ‰ RESUMEN FINAL - Job {job_id}:")
            logger.info(f"   ğŸ“„ Documento: {document_info['filename']} ({file_size_mb:.2f} MB)")
            logger.info(f"   ğŸ¤– IA: {request.processing_mode} - {len(extraction_result.extraction_data)} campos")
            logger.info(f"   â±ï¸ Tiempo: {processing_time_ms}ms")
            logger.info(f"   ğŸ—„ï¸ Cosmos DB: Document={doc_id}, Extraction={ext_id if 'ext_id' in locals() else 'N/A'}, Job={job_db_id if 'job_db_id' in locals() else 'N/A'}")
            
            # ===== INICIO: LIMPIEZA AUTOMÃTICA SEGÃšN PERSISTENCIA =====
            logger.info(f"ğŸ§¹ INICIANDO LIMPIEZA AUTOMÃTICA para job {job_id}")
            await self._cleanup_document_if_needed(job_id, blob_path, request.persistencia)
            logger.info(f"ğŸ§¹ LIMPIEZA AUTOMÃTICA COMPLETADA para job {job_id}")
            # ===== FIN: LIMPIEZA AUTOMÃTICA SEGÃšN PERSISTENCIA =====
            
            logger.info(f"âœ… Procesamiento sÃ­ncrono completado exitosamente para job {job_id}")
            return response
            
        except Exception as e:
            logger.error(f"âŒ Error en procesamiento sÃ­ncrono {job_id}: {str(e)}")
            raise
    
    async def _process_asynchronously(
        self,
        job_id: str,
        request: DocumentProcessingRequest,
        file_size_mb: float,
        start_time: float
    ) -> DocumentProcessingResponse:
        """Procesar documento de forma asÃ­ncrona"""
        logger.info(f"ğŸ”„ Iniciando procesamiento asÃ­ncrono para job {job_id}")
        
        try:
            # Guardar documento en Blob Storage (Capa Bronce)
            # Leer el archivo y subirlo usando BlobStorageService
            import os
            if os.path.exists(str(request.document_path)):
                with open(str(request.document_path), 'rb') as file:
                    file_content = file.read()
                    filename = os.path.basename(str(request.document_path))
                    
                    # Limpiar metadatos para Azure (convertir todos los valores a string)
                    clean_metadata = {}
                    if request.metadata:
                        for key, value in request.metadata.items():
                            if value is not None:
                                clean_metadata[key] = str(value)
                    
                    blob_path = await self.blob_storage_service.upload_document(
                        file_content=file_content,
                        filename=filename,
                        job_id=job_id,
                        metadata=clean_metadata
                    )
                    
                    if not blob_path:
                        raise Exception("Error subiendo documento a Blob Storage")
                        
                    logger.info(f"ğŸ“¦ Documento guardado en Blob Storage: {blob_path}")
            else:
                raise Exception(f"Archivo no encontrado: {request.document_path}")
            
            # Crear mensaje en la cola
            queue_message = {
                "job_id": job_id,
                "document_path": str(request.document_path),
                "blob_path": blob_path,
                "processing_mode": request.processing_mode,
                "prompt_general": request.prompt_general,
                "fields": [field.dict() for field in request.fields],
                "metadata": request.metadata,
                "file_size_mb": file_size_mb,
                "persistencia": str(request.persistencia).lower(),  # Forzar como string para evitar problemas de serializaciÃ³n
                "created_at": datetime.utcnow().isoformat()
            }
            
            # Logging detallado del mensaje de la cola
            logger.info(f"ğŸ“¬ PREPARANDO MENSAJE PARA LA COLA:")
            logger.info(f"   ğŸ¯ Persistencia original: {request.persistencia} (tipo: {type(request.persistencia)})")
            logger.info(f"   ğŸ”„ Persistencia convertida: {queue_message['persistencia']} (tipo: {type(queue_message['persistencia'])})")
            logger.info(f"   ğŸ“‹ Mensaje completo: {queue_message}")
            
            await self.queue_service.send_message(
                message_data=queue_message,
                queue_name=settings.azure_storage_queue_name
            )
            logger.info(f"ğŸ“¬ Mensaje encolado para job {job_id}")
            
            # Crear response de aceptaciÃ³n
            response = DocumentProcessingResponse(
                job_id=job_id,
                extraction_data=[],  # VacÃ­o para procesamiento asÃ­ncrono
                processing_summary=ProcessingSummary(
                    processing_status=ProcessingStatus.PENDING,
                    processing_time_ms=0,
                    file_size_mb=file_size_mb,
                    strategy_used=ProcessingMode(request.processing_mode),
                    timestamp=datetime.utcnow(),
                    pages_processed=0,
                    review_flags=[]
                ),
                message="Documento aceptado para procesamiento asÃ­ncrono",
                correlation_id=request.metadata.get("correlation_id") if request.metadata else None
            )
            
            # ===== INICIO: GUARDADO EN COSMOS DB =====
            logger.info(f"ğŸ—„ï¸ GUARDANDO JOB ASÃNCRONO EN COSMOS DB")
            
            # Crear informaciÃ³n del job para Cosmos DB
            job_info = {
                "job_id": job_id,
                "document_name": request.metadata.get("filename", "unknown") if request.metadata else "unknown",
                "processing_mode": request.processing_mode,
                "status": "pending",
                "created_at": datetime.utcnow().isoformat(),
                "file_size_mb": file_size_mb,
                "blob_path": blob_path,
                "fields_count": len(request.fields),
                "prompt_length": len(request.prompt_general) if request.prompt_general else 0
            }
            
            logger.info(f"   ğŸ“‹ InformaciÃ³n del job preparada:")
            logger.info(f"      ğŸ†” Job ID: {job_id}")
            logger.info(f"      ğŸ“„ Documento: {job_info['document_name']}")
            logger.info(f"      ğŸ¯ Modo: {request.processing_mode}")
            logger.info(f"      ğŸ“ TamaÃ±o: {file_size_mb:.2f} MB")
            logger.info(f"      ğŸ”— Blob Path: {blob_path}")
            logger.info(f"      ğŸ” Campos: {len(request.fields)}")
            
            # Crear job en Cosmos DB
            logger.info(f"   ğŸ—„ï¸ Creando job en Cosmos DB...")
            try:
                job_db_id = await self.cosmos_service.create_processing_job(job_info)
                if job_db_id:
                    logger.info(f"   âœ… Job creado exitosamente en Cosmos DB")
                    logger.info(f"      ğŸ†” Job DB ID: {job_db_id}")
                    logger.info(f"      ğŸ“ Container: processing_jobs")
                else:
                    logger.error(f"   âŒ Error: create_processing_job retornÃ³ None")
                    raise Exception("Error creando job en Cosmos DB")
            except Exception as cosmos_error:
                logger.error(f"   âŒ Error creando job en Cosmos DB: {str(cosmos_error)}")
                logger.error(f"   ğŸ” Detalles del error: {type(cosmos_error).__name__}")
                raise Exception(f"Error creando job en Cosmos DB: {str(cosmos_error)}")
            
            # Guardar estado inicial en Cosmos DB (mÃ©todo alternativo)
            try:
                await self.storage_service.save_job_status(job_id, ProcessingStatus.PENDING)
                logger.info(f"   ğŸ’¾ Estado inicial guardado en Cosmos DB para job {job_id}")
            except Exception as status_error:
                logger.warning(f"   âš ï¸ No se pudo guardar estado inicial: {str(status_error)}")
                logger.warning(f"   ğŸ” Continuando sin guardar estado inicial")
            
            logger.info(f"   " + "="*80)
            # ===== FIN: GUARDADO EN COSMOS DB =====
            
            logger.info(f"âœ… Procesamiento asÃ­ncrono iniciado para job {job_id}")
            return response
            
        except Exception as e:
            logger.error(f"âŒ Error en procesamiento asÃ­ncrono {job_id}: {str(e)}")
            raise
    
    async def get_document_status(self, job_id: str) -> Dict[str, Any]:
        """Obtener el estado de un documento en procesamiento"""
        logger.info(f"ğŸ” Consultando estado del job: {job_id}")
        
        try:
            status = await self.storage_service.get_job_status(job_id)
            logger.info(f"ğŸ“Š Estado del job {job_id}: {status}")
            return status
        except Exception as e:
            logger.error(f"âŒ Error consultando estado del job {job_id}: {str(e)}")
            raise
    
    async def get_document_result(self, job_id: str) -> Dict[str, Any]:
        """Obtener el resultado de un documento procesado"""
        logger.info(f"ğŸ” Consultando resultado del job: {job_id}")
        
        try:
            result = await self.storage_service.get_extraction_result(job_id)
            logger.info(f"ğŸ“Š Resultado del job {job_id} obtenido exitosamente")
            return result
        except Exception as e:
            logger.error(f"âŒ Error consultando resultado del job {job_id}: {str(e)}")
            raise
    
    def _validate_document_format(self, document_path: str) -> bool:
        """Validar si el formato del documento es soportado"""
        # Si es una ruta local (no URL), extraer solo el nombre del archivo
        if document_path.startswith('temp://') or '/' in document_path or '\\' in document_path:
            # Es una ruta local, extraer solo el nombre del archivo
            import os
            filename = os.path.basename(document_path)
            file_path = filename.lower()
        else:
            # Es una URL, parsear normalmente
            parsed_url = urlparse(document_path)
            file_path = parsed_url.path.lower()
        
        supported_formats = [
            '.pdf', '.png', '.jpg', '.jpeg', '.tiff', '.bmp',
            '.docx', '.doc', '.txt', '.rtf', '.pptx', '.ppt'
        ]
        
        is_supported = any(file_path.endswith(fmt) for fmt in supported_formats)
        logger.info(f"ğŸ” ValidaciÃ³n de formato: {file_path} - {'âœ… Soportado' if is_supported else 'âŒ No soportado'}")
        
        return is_supported
    
    async def _get_document_size(self, document_path: str) -> float:
        """Obtener el tamaÃ±o del documento desde la URL o archivo local"""
        try:
            # Si es una ruta local, obtener tamaÃ±o del archivo
            if document_path.startswith('temp://') or '/' in document_path or '\\' in document_path:
                import os
                if os.path.exists(document_path):
                    file_size_bytes = os.path.getsize(document_path)
                    file_size_mb = file_size_bytes / (1024 * 1024)
                    logger.info(f"ğŸ“ TamaÃ±o del archivo local: {file_size_mb:.2f} MB")
                    return file_size_mb
                else:
                    logger.warning(f"âš ï¸ Archivo local no encontrado: {document_path}")
                    return 5.0
            else:
                # Es una URL, usar la funciÃ³n original
                file_size_mb = await get_file_size_from_url(document_path)
                logger.info(f"ğŸ“ TamaÃ±o del documento obtenido: {file_size_mb:.2f} MB")
                return file_size_mb
        except Exception as e:
            logger.warning(f"âš ï¸ No se pudo obtener el tamaÃ±o del documento: {str(e)}")
            # Valor por defecto para continuar el procesamiento
            return 5.0
    
    async def _download_document(self, document_url: str) -> bytes:
        """Descargar el documento desde la URL"""
        logger.info(f"ğŸ“¥ Descargando documento desde: {document_url}")
        
        async with httpx.AsyncClient() as client:
            response = await client.get(document_url)
            response.raise_for_status()
            
            content = response.content
            logger.info(f"ğŸ“¥ Documento descargado: {len(content)} bytes")
            return content

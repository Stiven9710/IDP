#!/usr/bin/env python3
"""
Background Worker para procesar la cola de Azure Storage
Procesa documentos de forma as√≠ncrona usando Document Intelligence + GPT-4o
"""

import asyncio
import logging
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional

from app.services.queue_storage_service import QueueStorageService
from app.services.blob_storage_service import BlobStorageService
from app.services.cosmos_service import CosmosService
from app.services.ai_orchestrator import AIOrchestratorService
from app.core.config import settings

# Configurar logging
logger = logging.getLogger(__name__)


class BackgroundWorker:
    """Worker para procesar documentos de forma as√≠ncrona"""
    
    def __init__(self):
        """Inicializar el Background Worker"""
        self.queue_service = QueueStorageService()
        self.blob_service = BlobStorageService()
        self.cosmos_service = CosmosService()
        self.ai_orchestrator = AIOrchestratorService()
        
        # Configuraci√≥n del worker
        self.polling_interval = 5  # segundos entre consultas a la cola
        self.max_processing_time = 300  # segundos m√°ximo por documento
        self.is_running = False
        
        logger.info("üöÄ BackgroundWorker inicializado correctamente")
    
    async def start(self):
        """Iniciar el worker"""
        logger.info("üöÄ Iniciando Background Worker...")
        self.is_running = True
        
        try:
            while self.is_running:
                await self._process_queue()
                await asyncio.sleep(self.polling_interval)
                
        except KeyboardInterrupt:
            logger.info("üõë Background Worker detenido por el usuario")
        except Exception as e:
            logger.error(f"‚ùå Error en Background Worker: {str(e)}")
        finally:
            self.is_running = False
    
    async def stop(self):
        """Detener el worker"""
        logger.info("üõë Deteniendo Background Worker...")
        self.is_running = False
    
    async def _process_queue(self):
        """Procesar mensajes de la cola"""
        try:
            # Recibir mensaje de la cola espec√≠fica
            logger.info(f"üì• Intentando recibir mensaje de la cola: {settings.azure_storage_queue_name}")
            messages = await self.queue_service.receive_message(
                queue_name=settings.azure_storage_queue_name,  # Especificar cola correcta
                max_messages=1,
                visibility_timeout=60  # 1 minuto de visibilidad
            )
            
            if not messages:
                logger.info(f"üì≠ No hay mensajes en la cola {settings.azure_storage_queue_name}")
                return  # No hay mensajes para procesar
            
            logger.info(f"‚úÖ Mensaje recibido exitosamente de la cola {settings.azure_storage_queue_name}")
            
            message = messages[0]
            message_id = message.get('message_id')
            pop_receipt = message.get('pop_receipt')
            content = message.get('content', {})
            
            logger.info(f"üì• Procesando mensaje: {message_id}")
            logger.info(f"   üìÑ Documento: {content.get('filename', 'N/A')}")
            logger.info(f"   üéØ Modo: {content.get('processing_mode', 'N/A')}")
            
            try:
                # Procesar el documento
                await self._process_document(message_id, content)
                
                # Eliminar mensaje de la cola (procesado exitosamente)
                await self.queue_service.delete_message(message_id, pop_receipt)
                logger.info(f"‚úÖ Mensaje procesado y eliminado de la cola: {message_id}")
                
            except Exception as e:
                logger.error(f"‚ùå Error procesando mensaje {message_id}: {str(e)}")
                
                # Mover mensaje a cola de fallidos
                await self.queue_service.move_to_failed_queue(content, str(e))
                
                # Eliminar mensaje de la cola principal
                await self.queue_service.delete_message(message_id, pop_receipt)
                
                # Actualizar estado del job en Cosmos DB
                await self._update_job_status(content.get('job_id'), 'failed', str(e))
                
        except Exception as e:
            logger.error(f"‚ùå Error en _process_queue: {str(e)}")
    
    async def _process_document(self, message_id: str, content: Dict[str, Any]):
        """Procesar un documento espec√≠fico"""
        job_id = content.get('job_id')
        blob_path = content.get('blob_path')  # Cambiar blob_url por blob_path
        processing_mode = content.get('processing_mode', 'dual_service')
        prompt_general = content.get('prompt_general', '')
        fields = content.get('fields', [])
        
        logger.info(f"üîÑ INICIANDO PROCESAMIENTO DEL DOCUMENTO")
        logger.info(f"   üÜî Job ID: {job_id}")
        logger.info(f"   üìÑ Filename: {content.get('filename', 'unknown')}")
        logger.info(f"   üéØ Processing Mode: {processing_mode}")
        logger.info(f"   üìè File Size: {content.get('file_size_mb', 0)} MB")
        logger.info(f"   üîó Blob Path: {blob_path}")
        logger.info(f"   üìù Prompt Length: {len(prompt_general)} chars")
        logger.info(f"   üîç Fields Count: {len(fields)}")
        logger.info(f"   üÜî Message ID: {message_id}")
        logger.info(f"   üïê Timestamp: {datetime.utcnow().isoformat()}")
        logger.info(f"   " + "="*80)
        
        try:
            # 0. VERIFICAR ESTADO DEL JOB ANTES DE PROCESAR
            logger.info(f"üîí PASO 0: VERIFICANDO ESTADO DEL JOB")
            job_status = await self._get_job_status(job_id)
            
            if job_status in ['completed', 'failed']:
                logger.warning(f"   ‚ö†Ô∏è Job {job_id} ya est√° en estado '{job_status}', saltando procesamiento")
                return
            
            if job_status == 'processing':
                logger.warning(f"   ‚ö†Ô∏è Job {job_id} ya est√° siendo procesado, saltando procesamiento")
                return
            
            if job_status != 'pending':
                logger.warning(f"   ‚ö†Ô∏è Job {job_id} tiene estado inesperado '{job_status}', saltando procesamiento")
                return
            
            logger.info(f"   ‚úÖ Job {job_id} est√° en estado 'pending', procediendo con el procesamiento")
            
            # 1. Actualizar estado del job a 'processing'
            await self._update_job_status(job_id, 'processing')
            
            # 2. Descargar documento desde Blob Storage
            logger.info(f"üì• PASO 2: DESCARGANDO DOCUMENTO DESDE BLOB STORAGE")
            logger.info(f"   üîó Blob Path: {blob_path}")
            logger.info(f"   üïê Inicio descarga: {datetime.utcnow().isoformat()}")
            
            # ‚è±Ô∏è RETRASO OPTIMIZADO: Solo esperar estabilidad m√≠nima
            from app.core.config import settings
            logger.info(f"   ‚è±Ô∏è RETRASO OPTIMIZADO: Esperando {settings.azure_blob_retry_delay_seconds} segundos para estabilidad...")
            logger.info(f"   üìä Configuraci√≥n optimizada:")
            logger.info(f"      ‚è±Ô∏è Retraso inicial: {settings.azure_blob_retry_delay_seconds}s")
            logger.info(f"      üîÑ M√°ximo reintentos: {settings.azure_blob_max_retries}")
            logger.info(f"      üìà Factor backoff: {settings.azure_blob_retry_backoff_factor}x")
            logger.info(f"      ‚è∞ Tiempo total m√°ximo estimado: ~{settings.azure_blob_retry_delay_seconds * (1 + sum(settings.azure_blob_retry_backoff_factor ** i for i in range(settings.azure_blob_max_retries - 1)))}s")
            
            import asyncio
            await asyncio.sleep(settings.azure_blob_retry_delay_seconds)
            logger.info(f"   ‚úÖ Retraso inicial completado, iniciando descarga del documento...")
            
            # üì• DESCARGAR DOCUMENTO DEL CONTAINER ORIGINAL
            document_content = None
            total_wait_time = 0
            
            for attempt in range(settings.azure_blob_max_retries):
                logger.info(f"   üîÑ INTENTO {attempt + 1}/{settings.azure_blob_max_retries}")
                logger.info(f"      ‚è∞ Tiempo total esperado hasta ahora: {total_wait_time}s")
                
                # Intentar descargar del container original
                logger.info(f"   üì• Intentando descargar del container original...")
                try:
                    document_content = await self.blob_service.download_document(blob_path)
                    
                    if document_content:
                        logger.info(f"   ‚úÖ Descarga exitosa del container original en intento {attempt + 1}")
                        logger.info(f"      üéØ Tiempo total de espera: {total_wait_time}s")
                        break
                    else:
                        logger.warning(f"   ‚ö†Ô∏è Descarga retorn√≥ None del container original en intento {attempt + 1}")
                        
                except Exception as download_error:
                    logger.warning(f"   ‚ö†Ô∏è Error en intento {attempt + 1}: {str(download_error)}")
                    
                    if attempt < settings.azure_blob_max_retries - 1:
                        # Calcular retraso exponencial
                        delay = int(settings.azure_blob_retry_delay_seconds * (settings.azure_blob_retry_backoff_factor ** attempt))
                        total_wait_time += delay
                        
                        logger.info(f"   ‚è±Ô∏è Esperando {delay} segundos antes del siguiente intento...")
                        logger.info(f"      üìä Pr√≥ximo intento en: {delay}s")
                        logger.info(f"      ‚è∞ Tiempo total de espera: {total_wait_time}s")
                        
                        await asyncio.sleep(delay)
                    else:
                        # √öltimo intento fall√≥
                        logger.error(f"   ‚ùå Todos los intentos fallaron despu√©s de {total_wait_time}s de espera")
                        raise download_error
                

                
            # Verificar que se descarg√≥ correctamente
            if not document_content:
                logger.error(f"   ‚ùå Documento descargado est√° vac√≠o o es None")
                raise Exception("Documento descargado est√° vac√≠o o es None")
            
            logger.info(f"   ‚úÖ Descarga completada: {len(document_content)} bytes")
            logger.info(f"   üïê Fin descarga: {datetime.utcnow().isoformat()}")
            logger.info(f"   üìä Tama√±o del documento: {len(document_content)} bytes")
            logger.info(f"   " + "="*80)
            
            # 3. Procesar con IA
            logger.info(f"ü§ñ PASO 3: PROCESANDO DOCUMENTO CON IA")
            logger.info(f"   üéØ Processing Mode: {processing_mode}")
            logger.info(f"   üìù Prompt Length: {len(prompt_general)} chars")
            logger.info(f"   üîç Fields Count: {len(fields)}")
            logger.info(f"   üïê Inicio IA: {datetime.utcnow().isoformat()}")
            
            start_time = time.time()
            
            # Convertir campos de diccionario a objetos FieldDefinition
            from app.models.request import FieldDefinition
            logger.info(f"   üîÑ Convirtiendo campos a FieldDefinition...")
            field_definitions = [
                FieldDefinition(
                    name=field.get('name', ''),
                    type=field.get('type', 'string'),
                    description=field.get('description', '')
                )
                for field in fields
            ]
            logger.info(f"   ‚úÖ Campos convertidos: {len(field_definitions)}")
            
            # Procesar con AI Orchestrator
            logger.info(f"   üöÄ Llamando a AI Orchestrator...")
            try:
                extraction_result = await self.ai_orchestrator.process_document(
                    document_content=document_content,
                    fields=field_definitions,
                    prompt_general=prompt_general,
                    processing_mode=processing_mode
                )
                logger.info(f"   ‚úÖ AI Orchestrator completado exitosamente")
            except Exception as ai_error:
                logger.error(f"   ‚ùå Error en AI Orchestrator: {str(ai_error)}")
                logger.error(f"   üîç Detalles del error: {type(ai_error).__name__}")
                raise Exception(f"Error en AI Orchestrator: {str(ai_error)}")
            
            processing_time_ms = int((time.time() - start_time) * 1000)
            logger.info(f"   ‚è±Ô∏è Tiempo de procesamiento IA: {processing_time_ms}ms")
            logger.info(f"   üìä Campos extra√≠dos: {len(extraction_result.extraction_data)}")
            logger.info(f"   üìÑ P√°ginas procesadas: {extraction_result.pages_processed or 0}")
            logger.info(f"   üïê Fin IA: {datetime.utcnow().isoformat()}")
            logger.info(f"   " + "="*80)
            
            # 4. Guardar resultado en Cosmos DB
            logger.info(f"üóÑÔ∏è PASO 4: GUARDANDO RESULTADO EN COSMOS DB")
            logger.info(f"   üïê Inicio guardado: {datetime.utcnow().isoformat()}")
            
            # Guardar informaci√≥n del documento
            logger.info(f"   üìÑ Preparando informaci√≥n del documento...")
            document_info = {
                "filename": content.get('filename', 'unknown'),
                "file_size_mb": content.get('file_size_mb', 0),
                "file_type": content.get('filename', '').split('.')[-1] if '.' in content.get('filename', '') else 'unknown',
                "status": "processed",
                "processing_mode": processing_mode,
                "job_id": job_id,
                "created_at": datetime.utcnow().isoformat(),
                "ai_processing_time_ms": processing_time_ms
            }
            logger.info(f"   ‚úÖ Informaci√≥n del documento preparada")
            logger.info(f"   üìä Campos del documento: {len(document_info)}")
            
            logger.info(f"   üóÑÔ∏è Guardando documento en Cosmos DB...")
            try:
                doc_id = await self.cosmos_service.save_document(document_info)
                if doc_id:
                    logger.info(f"   ‚úÖ Documento guardado en Cosmos DB: {doc_id}")
                else:
                    logger.error(f"   ‚ùå Error: save_document retorn√≥ None")
                    raise Exception("Error guardando documento en Cosmos DB")
            except Exception as doc_error:
                logger.error(f"   ‚ùå Error guardando documento: {str(doc_error)}")
                logger.error(f"   üîç Detalles del error: {type(doc_error).__name__}")
                raise Exception(f"Error guardando documento en Cosmos DB: {str(doc_error)}")
            
            logger.info(f"   " + "="*80)
            
            # Guardar resultado de extracci√≥n
            logger.info(f"   üîç Preparando resultado de extracci√≥n...")
            extraction_data = {
                "document_id": doc_id if doc_id else None,
                "job_id": job_id,
                "extraction_date": datetime.utcnow().isoformat(),
                "processing_time_ms": processing_time_ms,
                "strategy_used": processing_mode,
                "extraction_data": [field.dict() for field in extraction_result.extraction_data],
                "processing_summary": {
                    "processing_status": "completed",
                    "strategy_used": processing_mode,
                    "processing_time_ms": processing_time_ms,
                    "pages_processed": extraction_result.pages_processed or 0
                }
            }
            logger.info(f"   ‚úÖ Resultado de extracci√≥n preparado")
            logger.info(f"   üìä Campos de extracci√≥n: {len(extraction_data)}")
            logger.info(f"   üîç Datos extra√≠dos: {len(extraction_data['extraction_data'])} campos")
            
            logger.info(f"   üóÑÔ∏è Guardando resultado de extracci√≥n en Cosmos DB...")
            try:
                ext_id = await self.cosmos_service.save_extraction_result(extraction_data)
                if ext_id:
                    logger.info(f"   ‚úÖ Resultado de extracci√≥n guardado en Cosmos DB: {ext_id}")
                else:
                    logger.error(f"   ‚ùå Error: save_extraction_result retorn√≥ None")
                    raise Exception("Error guardando resultado de extracci√≥n en Cosmos DB")
            except Exception as ext_error:
                logger.error(f"   ‚ùå Error guardando extracci√≥n: {str(ext_error)}")
                logger.error(f"   üîç Detalles del error: {type(ext_error).__name__}")
                raise Exception(f"Error guardando resultado de extracci√≥n en Cosmos DB: {str(ext_error)}")
            
            logger.info(f"   " + "="*80)
            
            # 5. Actualizar estado del job a 'completed'
            logger.info(f"üîÑ PASO 5: ACTUALIZANDO ESTADO DEL JOB A 'COMPLETED'")
            logger.info(f"   üÜî Job ID: {job_id}")
            logger.info(f"   ‚è±Ô∏è Processing Time: {processing_time_ms}ms")
            logger.info(f"   üîç Fields Extracted: {len(extraction_result.extraction_data)}")
            logger.info(f"   üóÑÔ∏è Document ID: {doc_id}")
            logger.info(f"   üîç Extraction ID: {ext_id}")
            
            try:
                await self._update_job_status(
                    job_id, 
                    'completed', 
                    processing_time_ms=processing_time_ms,
                    fields_extracted=len(extraction_result.extraction_data),
                    document_id=doc_id,
                    extraction_id=ext_id
                )
                logger.info(f"   ‚úÖ Estado del job actualizado exitosamente")
            except Exception as status_error:
                logger.error(f"   ‚ùå Error actualizando estado del job: {str(status_error)}")
                logger.error(f"   üîç Detalles del error: {type(status_error).__name__}")
                raise Exception(f"Error actualizando estado del job: {str(status_error)}")
            
            # 6. Mover documento procesado al contenedor de procesados
            logger.info(f"üìÅ PASO 6: MOVIENDO DOCUMENTO A CONTENEDOR PROCESADO")
            logger.info(f"   üîó Blob Path: {blob_path}")
            logger.info(f"   üÜî Job ID: {job_id}")
            
            move_successful = False
            try:
                await self.blob_service.move_to_processed(blob_path, job_id, {
                    "status": "completed",
                    "processing_time_ms": processing_time_ms,
                    "fields_extracted": len(extraction_result.extraction_data)
                })
                logger.info(f"   ‚úÖ Documento movido a contenedor procesado exitosamente")
                move_successful = True
            except Exception as move_error:
                logger.error(f"   ‚ùå Error moviendo documento: {str(move_error)}")
                logger.error(f"   üîç Detalles del error: {type(move_error).__name__}")
                logger.warning(f"   ‚ö†Ô∏è Continuando sin mover documento")
                # No marcar como failed si solo fall√≥ el movimiento
            
            # 7. Actualizar estado final del job
            logger.info(f"üîÑ PASO 7: ACTUALIZANDO ESTADO FINAL DEL JOB")
            try:
                if move_successful:
                    # Si el movimiento fue exitoso, marcar como completed
                    await self._update_job_status(
                        job_id, 
                        'completed', 
                        processing_time_ms=processing_time_ms,
                        fields_extracted=len(extraction_result.extraction_data),
                        document_id=doc_id,
                        extraction_id=ext_id
                    )
                    logger.info(f"   ‚úÖ Job marcado como 'completed'")
                else:
                    # Si el movimiento fall√≥ pero el procesamiento fue exitoso, marcar como completed con warning
                    await self._update_job_status(
                        job_id, 
                        'completed', 
                        processing_time_ms=processing_time_ms,
                        fields_extracted=len(extraction_result.extraction_data),
                        document_id=doc_id,
                        extraction_id=ext_id
                    )
                    logger.warning(f"   ‚ö†Ô∏è Job marcado como 'completed' (movimiento fall√≥ pero procesamiento exitoso)")
            except Exception as status_error:
                logger.error(f"   ‚ùå Error actualizando estado final del job: {str(status_error)}")
            
            # 8. Resumen final
            logger.info(f"üéâ PROCESAMIENTO COMPLETADO EXITOSAMENTE")
            logger.info(f"   üÜî Job ID: {job_id}")
            logger.info(f"   üìÑ Filename: {content.get('filename', 'unknown')}")
            logger.info(f"   üìä Campos extra√≠dos: {len(extraction_result.extraction_data)}")
            logger.info(f"   ‚è±Ô∏è Tiempo total: {processing_time_ms}ms")
            logger.info(f"   üóÑÔ∏è Document ID: {doc_id}")
            logger.info(f"   üîç Extraction ID: {ext_id}")
            logger.info(f"   üìÅ Movimiento a processed: {'‚úÖ Exitoso' if move_successful else '‚ö†Ô∏è Fall√≥ pero continu√≥'}")
            logger.info(f"   üïê Timestamp final: {datetime.utcnow().isoformat()}")
            logger.info(f"   " + "="*80)
            
        except Exception as e:
            logger.error(f"‚ùå ERROR PROCESANDO DOCUMENTO")
            logger.error(f"   üÜî Job ID: {job_id}")
            logger.error(f"   üìÑ Filename: {content.get('filename', 'unknown')}")
            logger.error(f"   ‚ùå Error: {str(e)}")
            logger.error(f"   üîç Tipo de error: {type(e).__name__}")
            logger.error(f"   üïê Timestamp del error: {datetime.utcnow().isoformat()}")
            logger.error(f"   " + "="*80)
            
            # Actualizar estado del job a 'failed'
            logger.info(f"üîÑ Actualizando estado del job a 'failed'...")
            try:
                await self._update_job_status(job_id, 'failed', str(e))
                logger.info(f"   ‚úÖ Estado del job actualizado a 'failed'")
            except Exception as update_error:
                logger.error(f"   ‚ùå Error actualizando estado del job: {str(update_error)}")
            
            # Re-lanzar excepci√≥n para que se maneje en _process_queue
            raise
    
    async def _get_job_status(self, job_id: str) -> str:
        """Obtener el estado actual de un job desde Cosmos DB"""
        try:
            logger.info(f"   üîç Consultando estado del job {job_id} en Cosmos DB...")
            
            # Obtener el job desde Cosmos DB
            job = await self.cosmos_service.get_processing_job(job_id)
            
            if job:
                status = job.get('status', 'unknown')
                logger.info(f"   ‚úÖ Estado del job {job_id}: {status}")
                logger.info(f"      üìä Datos del job: {list(job.keys())}")
                return status
            else:
                logger.warning(f"   ‚ö†Ô∏è Job {job_id} no encontrado en Cosmos DB")
                return 'not_found'
                
        except Exception as e:
            logger.error(f"   ‚ùå Error consultando estado del job {job_id}: {str(e)}")
            return 'error'
    
    async def _update_job_status(
        self, 
        job_id: str, 
        status: str, 
        error_message: str = None,
        processing_time_ms: int = None,
        fields_extracted: int = None,
        document_id: str = None,
        extraction_id: str = None
    ):
        """Actualizar el estado de un job en Cosmos DB"""
        try:
            # Buscar el job existente
            existing_job = await self.cosmos_service.get_processing_job(job_id)
            
            if existing_job:
                # Actualizar job existente
                update_data = {
                    "status": status,
                    "updated_at": datetime.utcnow().isoformat()
                }
                
                if status == "completed":
                    update_data.update({
                        "completed_at": datetime.utcnow().isoformat(),
                        "processing_time_ms": processing_time_ms or 0,
                        "fields_extracted": fields_extracted or 0,
                        "document_id": document_id,
                        "extraction_id": extraction_id
                    })
                elif status == "failed":
                    update_data.update({
                        "failed_at": datetime.utcnow().isoformat(),
                        "error_message": error_message
                    })
                elif status == "processing":
                    update_data.update({
                        "started_at": datetime.utcnow().isoformat()
                    })
                
                # Actualizar en Cosmos DB
                logger.info(f"   üîÑ Llamando a CosmosService.update_job_status...")
                logger.info(f"      üìä Par√°metros: job_id={job_id}, status={status}, update_data={update_data}")
                
                result = await self.cosmos_service.update_job_status(job_id, status, update_data)
                
                if result:
                    logger.info(f"‚úÖ Estado del job actualizado exitosamente: {job_id} -> {status}")
                else:
                    logger.error(f"‚ùå CosmosService.update_job_status retorn√≥ False para job: {job_id}")
                    raise Exception(f"Error actualizando estado del job en Cosmos DB: {job_id}")
                
            else:
                # Crear nuevo job si no existe
                job_info = {
                    "job_id": job_id,
                    "document_name": "unknown",
                    "processing_mode": "dual_service",
                    "status": status,
                    "created_at": datetime.utcnow().isoformat(),
                    "updated_at": datetime.utcnow().isoformat()
                }
                
                if status == "completed":
                    job_info.update({
                        "completed_at": datetime.utcnow().isoformat(),
                        "processing_time_ms": processing_time_ms or 0,
                        "fields_extracted": fields_extracted or 0
                    })
                
                await self.cosmos_service.create_processing_job(job_info)
                logger.info(f"‚úÖ Nuevo job creado: {job_id} con estado {status}")
                
        except Exception as e:
            logger.error(f"‚ùå Error actualizando estado del job {job_id}: {str(e)}")


async def start_worker():
    """Funci√≥n para iniciar el worker desde el main"""
    worker = BackgroundWorker()
    await worker.start()


if __name__ == "__main__":
    # Para pruebas independientes
    asyncio.run(start_worker())
